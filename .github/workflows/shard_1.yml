name: TradingView Scraper - Shard 1 (1251-2500)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *' # Runs daily at 00:00 UTC

jobs:
  scrape_shard_1:
    runs-on: ubuntu-latest
    timeout-minutes: 60 # Set a maximum runtime for safety

    steps:
      - name: 1. Checkout repository code
        uses: actions/checkout@v4

      - name: 2. Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: 3. Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests gspread selenium beautifulsoup4 openpyxl webdriver-manager

      - name: 4. Create secrets files (Credentials & Cookies)
        run: |
          echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json
          echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json
        env:
          GSPREAD_CREDENTIALS: ${{ secrets.GSPREAD_CREDENTIALS }}
          TRADINGVIEW_COOKIES: ${{ secrets.TRADINGVIEW_COOKIES }}

      - name: 5. Execute Python Scraper (SHARD 1)
        run: python run_scraper.py
        env:
          # --- SHARDING CONFIGURATION ---
          START_INDEX: 1251  # Starts at row 1251
          END_INDEX: 2500    # Scrapes up to row 2500
          SHARD_INDEX: 0     # Not using SHARD_INDEX/STEP in this split
          SHARD_STEP: 1
          CHECKPOINT_FILE: checkpoint_shard_1.txt # Unique checkpoint file for this job
