name: TradingView Data Scraper

on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
  # Sets the workflow to run on a schedule (e.g., daily at 00:00 UTC)
  schedule:
    - cron: '0 0 * * *' 
  # You can also trigger it on pushes to main if needed:
  # push:
  #   branches: [ main ]

jobs:
  scrape_and_upload:
    runs-on: ubuntu-latest

    steps:
      - name: 1. Checkout repository code
        uses: actions/checkout@v4

      - name: 2. Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Use a stable Python version

      - name: 3. Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install all required libraries, including webdriver-manager and gspread
          pip install pandas requests gspread selenium beautifulsoup4 openpyxl webdriver-manager

      - name: 4. Create secrets files (Credentials & Cookies)
        run: |
          # Creates the credentials.json file from the secret
          echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json
          
          # Creates the cookies.json file from the secret (required for login)
          echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json
        env:
          # Define required secrets which must be set in your GitHub repository secrets
          GSPREAD_CREDENTIALS: ${{ secrets.GSPREAD_CREDENTIALS }}
          TRADINGVIEW_COOKIES: ${{ secrets.TRADINGVIEW_COOKIES }}

      - name: 5. Execute the Python Scraper
        run: python run_scraper.py
        # Pass environment variables for sharding/control (optional)
        env:
          START_INDEX: 1
          END_INDEX: 2500
          SHARD_INDEX: 0 # Only running one shard in this example
          SHARD_STEP: 1
