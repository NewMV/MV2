name: Production Stock Scraper (2500 Symbols)

on:
  workflow_dispatch: # Allows you to start the scrape manually from the "Actions" tab
  schedule:
    - cron: '0 0 * * *' # Optional: Run automatically every day at midnight

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false # If one shard fails, don't stop the others
      matrix:
        # This creates 10 parallel runners
        shard: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Google Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install Python Dependencies
        run: |
          pip install gspread selenium beautifulsoup4 webdriver-manager

      - name: Create Secrets Files
        run: |
          # Uses your specific secret names to create the files the script needs
          echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json
          echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json

      - name: Run Scraper Shard ${{ matrix.shard }}
        env:
          START_INDEX: 1
          END_INDEX: 2500
          SHARD_INDEX: ${{ matrix.shard }}
          SHARD_STEP: 10 # This must match the number of shards in the matrix
        run: python run_scraper.py # Ensure this matches your filename
