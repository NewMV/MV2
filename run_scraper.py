import os
import time
import json
import gspread
from datetime import date
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager

# ---------------- SHARDING (main logic preserved) ---------------- #
SHARD_INDEX = int(os.getenv("SHARD_INDEX", "0"))
SHARD_STEP = int(os.getenv("SHARD_STEP", "1"))
START_INDEX = int(os.getenv("START_INDEX", "1"))
END_INDEX = int(os.getenv("END_INDEX", "2500"))
checkpoint_file = os.getenv("CHECKPOINT_FILE", "checkpoint_new_1.txt")
last_i = int(open(checkpoint_file).read()) if os.path.exists(checkpoint_file) else START_INDEX

# ---------------- SETUP ---------------- #
chrome_options = Options()
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# ---------------- GOOGLE SHEETS AUTH ---------------- #
try:
    gc = gspread.service_account("credentials.json")
    
    # 1. Open the SOURCE file and tab
    source_file = gc.open('Stock List')
    source_sheet = source_file.worksheet('Sheet1')
    
    # 2. Open the DESTINATION file and tab
    dest_file = gc.open('New MV2')
    output_sheet = dest_file.worksheet('Sheet5')
    
    # Read source data
    all_rows = source_sheet.get_all_values()
    data_rows = all_rows[1:]  # Skip header
    print(f"‚úÖ Connected! Reading from 'Stock List' [Sheet1] | Writing to 'New MV2' [Sheet5]")
except Exception as e:
    print(f"‚ùå Connection Error: {e}")
    print("üí° Tip: Make sure the service account email is shared with BOTH 'Stock List' and 'New MV2' spreadsheets.")
    exit(1)

current_date = date.today().strftime("%m/%d/%Y")

# ---------------- SCRAPER FUNCTION (main logic preserved) ---------------- #
def scrape_tradingview(company_url):
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=chrome_options
    )
    driver.set_window_size(1920, 1080)
    try:
        if os.path.exists("cookies.json"):
            driver.get("https://www.tradingview.com/")
            with open("cookies.json", "r", encoding="utf-8") as f:
                cookies = json.load(f)
            for cookie in cookies:
                try:
                    cookie_to_add = {k: cookie[k] for k in ('name', 'value', 'domain', 'path') if k in cookie}
                    driver.add_cookie(cookie_to_add)
                except: pass
            driver.refresh()
            time.sleep(2)

        driver.get(company_url)
        print(f"üîé Visiting: {company_url}")
        # Preserving your exact XPATH logic
        WebDriverWait(driver, 45).until(
            EC.visibility_of_element_located((By.XPATH, '/html/body/div[2]/div/div[5]/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/div[2]/div[2]/div[2]/div[2]/div'))
        )
        soup = BeautifulSoup(driver.page_source, "html.parser")
        values = [el.get_text().replace('‚àí', '-').replace('‚àÖ', '').strip() for el in soup.find_all("div", class_="valueValue-l31H9iuA apply-common-tooltip")]
        return values
    except Exception as e:
        print(f"‚ùå Scraping error: {e}")
        return []
    finally:
        driver.quit()

# ---------------- MAIN LOOP (logic preserved + batching) ---------------- #
results_batch = []
batch_start_row = None 

for i, row in enumerate(data_rows):
    # Original sharding logic preserved
    if i < last_i or i < START_INDEX or i > END_INDEX:
        continue
    if i % SHARD_STEP != SHARD_INDEX:
        continue

    # Input: Column A (0) is Name, Column D (3) is URL
    name = row[0] if len(row) > 0 else f"Row {i}"
    company_url = row[3] if len(row) > 3 else ""
    target_row = i + 2 
    
    if batch_start_row is None:
        batch_start_row = target_row

    print(f"üìå Index {i} | {name} | Target Row: {target_row}")
    values = scrape_tradingview(company_url)

    # Sequence logic: Ensure Symbol is always first; use "Error" if data missing
    if values:
        row_data = [name, current_date] + values
    else:
        # Maintaining alignment so your sheet sequence stays perfect
        row_data = [name, current_date, "Error", "Error", "Error", "Error"]

    results_batch.append(row_data)

    # Batch writing: Send 10 rows at once to avoid Google Sheets API limits
    if len(results_batch) >= 10:
        try:
            output_sheet.update(f'A{batch_start_row}', results_batch)
            print(f"üíæ Saved batch up to row {target_row}")
            results_batch = []
            batch_start_row = None
        except Exception as e:
            print(f"‚ö†Ô∏è Write Error: {e}")

    # Preserving your checkpoint system
    with open(checkpoint_file, "w") as f:
        f.write(str(i + 1))
    
    time.sleep(1)

# Upload any remaining items at the end
if results_batch:
    output_sheet.update(f'A{batch_start_row}', results_batch)

print("\nüèÅ Scraping process complete.")
